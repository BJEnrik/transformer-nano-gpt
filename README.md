# The Decoder Transformer: Implementing a nanoGPT with Andrej Karpathy

ChatGPT has revolutionized the world, ushering in the era of AI and generative models. At its core lies a powerful algorithm called Generative Pretrained Transformer (GPT), inspired by the groundbreaking research paper "Attention is All You Need" by Vaswani et al. GPT builds upon the successes of its predecessors, GPT-2 and GPT-3, both developed by openAI. These models have set new benchmarks in natural language processing and have showcased remarkable capabilities in generating human-like text. [1]

In this notebook, we embark on a coding journey alongside Andrej Karpathy, following his instructional video titled "Let's build GPT: from scratch, in code, spelled out." Our objective is to create a Decoder Transformer, also known as nanoGPT. The notebook is divided into three main parts:

1. Understanding the fundamentals of transformers and self-attention.
2. Implementing the complete nanoGPT code.
3. Fine-tuning nanoGPT to achieve lower loss and better results.

To facilitate understanding for our readers, we provide concise explanations in everyday language for each code cell. Furthermore, throughout the notebook, we address common questions about language models, aiming to enhance your comprehension of these powerful tools.

